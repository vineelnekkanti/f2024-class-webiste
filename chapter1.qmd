---
title: "Chapter 1: Manipulations"
author: "Biagio Palese"
date: "`r format(Sys.time(), '%d %B, %Y')`" 
format: 
  html:
    toc: TRUE
    toc_float: TRUE
    code_download: TRUE
---

```{r, echo=FALSE, results='asis'}
# Inject JavaScript to check authentication status and redirect if necessary
cat(htmltools::HTML('<script>
if (localStorage.getItem("isLoggedIn") !== "true") {
    window.location.href = "login.html";
}
</script>'))

```

```{r setup, include=FALSE, cache = F}
knitr::opts_chunk$set(
  echo = TRUE,
  error = TRUE,
  warning= FALSE,
  message= FALSE)
# install.packages("tidyverse")
# install.packages("nycflights13")
library(tidyverse)
library(nycflights13)
set.seed(0101)
flights <- sample_n(flights, 15000)
departments <- c("OMIS", "Finance", "Accounting" , "Management","Marketing")
students_number <- c(220, 180, 255, 100, 170)
students_avg_age <- c(21.5, 22.1, 21.8, 21.3, 21.5)
cob <- tibble(departments,students_number,students_avg_age)
```

## Manipulations

![When working on a new dataset the beginning might not be easy](images/course_images/manipulations.png)

The following sections of our book R for Data Science( [first portion of the course book](https://r4ds.hadley.nz) ) are included in the second week:

-   [Data Transformation](https://r4ds.hadley.nz/data-transform): Sections: from 3 to 3.3 & part of 3.5 included

### Link to other resources

-   Internal help: [posit support](https://support.posit.co/hc/en-us)

-   External help: [stackoverflow](https://stackoverflow.com/search?q=rstudio&s=25d8522e-3191-4bf2-ae3b-ccad762aeca9)

-   Additional materials: [posit resources](https://posit.co/resources/)

-   Cheat Sheets: [posit cheat sheets](https://posit.co/resources/cheatsheets/)

While I use the book as a reference the materials provided to you are custom made and include more activities and resources. If you understand the materials covered in this document there is no need to refer to other sources. If you have any troubles with the materials don't hesitate to contact me or check the above sources.

### Load packages

```{r}
library(tidyverse)
library(nycflights13)
```

![Data Science model: Artwork by @allison_horst](images/environmental-data-science-r4ds-general.png)

### Get to know your data

```{r know your data, exercise=FALSE}
flights#dataset we will use --> make sure you run library(nycflights13) before running this line of code
?flights # get a description of it
#view(flights)# run this if you want to explore the entire dataset --> which will open the dataset in the RStudio viewer.
glimpse(flights)# run this if you want a summary of the dataset 
colnames(flights)# know the columns name of your dataset
```

### Data types

Only the most used ones are covered below:

-   *int* stands for integers (1,2,3).

-   *dbl* stands for doubles, or real numbers (-1, 1.5,4/5).

-   *chr* stands for character vectors, or strings ("this is a string").

-   *fctr* stands for factors, which R uses to represent categorical variables with fixed possible values (freshman, sophomore, junior, senior).

-   *lgl* stands for logical, vectors that contain only TRUE or FALSE (True, False, True).

-   *date* stands for dates (01/18/2021).

-   *dttm* stands for date-times, a date + a time (01/18/2021 11:00 am).

### Data structures

Only the most used ones are covered below:

-   *Vector*: An atomic vector (or simply vector) is the simplest data structure in R which consists of an ordered set of values of the same type (e.g. numeric, character, date, etc…).

#### Examples:

```{r exampletibble1, exercise=T}
departments <- c("OMIS", "Finance", "Accounting" , "Management","Marketing")# a vector can be created using the combine function c()
#departments


```

```{r exampletibble2, exercise=T}
students_number <- c(220, 180, 255, 100, 170)#all the elements of a vector must have the same data type. This is why the data type determines the type of vector you have (e.g., numeric, characters, date vectors).
#students_number

```

```{r exampletibble3, exercise=T}
students_avg_age <- c(21.5, 22.1, 21.8, 21.3, 21.5)
#students_avg_age

```

-   *Dataframe/tibble/dataset*: A dataframe is a data structure that organizes data into a 2-dimensional table of rows and columns, much like a spreadsheet. Dataframes are called tibbles in R (tidyverse).

We can create a tibble by combining two or more vectors of the same length.

#### Example:

```{r exampletibble4, exercise=T}
cob <- tibble(departments,students_number,students_avg_age)# each vector was of 5 elements (or length 5) --> this is why the cob dataframe has 5 rows. We are using three vectors in our cob dataframe --> this is why we have 3 columns or variables. As you can see the vector name has become the column name and a tibble is not much different than an Excel spreadsheet
#cob
```

*NOTE*: there are other data structures available in R (e.g., matrices, lists etc.) but we will not use or cover them in this course.

Now that you have a basic understanding of data types and structures (we will cover more about tibble next week) we can dive into useful functions for wrangling your data.

### 5 + 1 Key data manipulation (dplyr package) functions:

-   Pick observations based on their values (*filter()*).

-   Reorder the rows/columns (*arrange()*).

-   Pick variables based their column names (*select()*).

-   Create new variables or update existing variables (*mutate()*).

-   Collapse many values down to a single summary (*summarise()*).

-   All the above functions can be used in conjunction with the function *group_by()* (our +1 function). group_by changes the scope of each function from operating on the entire dataset to operating on it group-by-group.

#### How they work?

These six functions provide the verbs for a language of data manipulation. All verbs work similarly, and this is a great news, and have a similar structure:

The first argument is a dataframe on which you want to perform a manipulation.

The subsequent arguments describe what do you want to do with the original dataframe, using the variable names (without quotes).

The result is a new dataframe (remember to assign it to a new object if you want to preserve the changes). Together these properties make it easy to chain together multiple simple steps to achieve a complex result.

## Filter()

**filter()** is used to include in your dataset only observations that meet one or more logical conditions. For example, you will use filter if from an imaginary US tax payers dataset (tax_payers), you want to continue your analysis only on tax payers that live in Illinois given a states variable (notice that by doing so \# of observations/rows decreases while \# of variables/columns stays the same). --\> **filter(tax_payers, state== "IL")**

```{r filter_examples1, exercise=T}
#Example: flights that traveled on January (Note the # of observation included in the dataset)
filter(#the results do not replace the original dataset. To save it to future analysis assign it to a new variable named jan_flights. Create the variable jan_flights 
 #<- 
# print jan_flights
#Example 2: flights that traveled to Ohare (ORD)

```

To use filtering effectively, you have to know how to select the observations that you want using the comparison operators. R provides the standard suite: \>, \>=, \<, \<=, != (not equal), and == (equal).

When you’re starting out with R, the easiest mistake to make is to use = instead of == when testing for equality. When this happens you’ll get an informative error:

```{r}
filter(flights, month = 1)
```

Multiple arguments to filter() are combined with “and”: every expression must be true in order for a row to be included in the output. For other types of combinations, you’ll need to use Boolean operators yourself: & is “and”, \| is “or”, and ! is “not”. The "," can also be used instead of &. However, I do recommend to use & especially at the beginning as it is easier for you to remind that both conditions must be met for an observation to be included in the analysis. However, if you combine an "&" and "\|" in the same filter you need parenthesis to separate the "&" and the "\|" (see below note).

```{r filter_examples2, exercise=T}
#Example 3: find flights in November and December
filter(# look at the results. Does it make sense to have such filter?

#Example 4: find flights in November or December
filter(# Now this makes sense as you want to constraint the focus of your analysis to the last two months of the year. You might have an intuition that during the holiday season there is need for more flights/personnel 

# Example 5: find flights that weren’t delayed (on arrival or departure) by more than two hours 
filter(# Hours are presented in minutes in the dataset that's why 120. Again, you need to get to know your data before starting manipulations. If you don't remember column names check the colnames function (?colnames).

```

### Activity 1: Filter

```{r activity-1a, exercise=TRUE}
#flights in january with more than 60 minutes delay

```

```{r activity-1b, exercise=TRUE}
#flights with departure or arrival delay smaller than 15 minutes

```

```{r activity-1c, exercise=TRUE}
#flights with distance equal or bigger than 1010 miles

```

```{r activity-1d, exercise=TRUE}
#flights operated by american airlines (AA)

```

#### NOTE

Be careful in using & and , as substitutes. In some cases they are but in other not. For example if you are looking for the flights in January with more than 60 minutes delay both in departure and arrival

```{r note-filter1, exercise=TRUE}
filter(flights, month==1 & dep_delay> 60 & arr_delay>60)

```

this line is equivalent to the one below with ,.

```{r note-filter2, exercise=TRUE}

filter(flights, month==1 , dep_delay> 60 , arr_delay>60)
 
```

In this case & and , are perfect substitutes but it is not always the case. Let's check the example below:

```{r note-filter3, exercise=TRUE}
filter(flights, month==1 & dep_delay> 60 |arr_delay>60)
```

```{r note-filter4, exercise=TRUE}
filter(flights, month==1 , dep_delay> 60 |arr_delay>60)
```

In fact, the \| is affecting the equivalency with the ,. So, if in your filter you have both a & and \|, the & and the , are not equivalent.

To achieve equality in this case, you need parenthesis to divide the & and \| conditions.

```{r note-filter5, exercise=TRUE}
filter(flights, month==1 & (dep_delay> 60 |arr_delay>60))
```

Without the parenthesis you will get different & wrong results as above. So, please be careful and keep this in mind.

## Arrange()

**arrange()** works similarly to filter() except that instead of selecting rows, it changes the order in which they are presented in the dataset. For example, using the above US tax payers dataset, you will use arrange if you want to sort the dataset by the tax payers last name in decreasing order (Z-A). --\> **arrange(tax_payers, desc(last_name))** Arrange takes a dataframe and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:

```{r arrange-example1, exercise=TRUE}
arrange(flights, year, month, day)
#arrange(flights, desc(dep_delay))#desc allows to re-order by a column in descending order
# Example 1: arrange the dataset based on destination increasing alphabetical order

#Example 2: arrange the dataset based on distance decreasing order

```

### Activity 2: Arrange

```{r activity-2a, exercise=TRUE}
#flights by distance from smaller to bigger

```

```{r activity-2b, exercise=TRUE}
#flights by air time

```

```{r activity-2c, exercise=TRUE}
#flights with distance equal or bigger than 1010 miles

```

```{r activity-2d, exercise=TRUE}
#flights by arr_delay from bigger to smaller

```

## Select()

**select()** allows you to continue the analysis only on some specific columns of your original dataset. By using select you can rapidly zoom in on a useful subset of variables that you think needs deeper investigation or that are part of the scope of your analysis. For example, using the same imaginary US tax payer dataset, you want to focus your analysis on just salary, gender and age. It is unnecessary to keep all the other columns as you already know that they are not going to be included in the scope of the analysis --\> **select(tax_payers, salary, gender, age)**

```{r select-example1, exercise=TRUE}
select(flights, year, month, day)#Select columns by name
select(flights, year:day)#Select all columns between year and day (inclusive)
select(flights, 1:3)#Select columns based on their position in the dataset (inclusive)
select(flights, -(year:day))# Select all columns except those from year to day (inclusive)

# Example 1: select the 2nd, 4th and 10th columns 

#Example 2: select all the columns but the last 3

```

As you can see there are multiple ways to select the same columns (first 3 lines of code give the same outcome: year, month , day subset), but not all of them have the same efficiency. Imagine selecting the first 15 column using the first line of code (you will have to manually type 15 columns name) compared to using the second line (first column : fifteenth column) or the third (1:15). However, when you use the last two methods I recommend to check the column order using the function *colnames()*

Moreover, there are a number of helper functions you can use within select():

*starts_with("abc")*: matches names that begin with “abc”.

*ends_with("xyz")*: matches names that end with “xyz”.

*contains("ijk")*: matches names that contain “ijk”.

See ?select for more details.

### Activity 3: Select

```{r activity-3a, exercise=TRUE}
#first 10 adjacent columns

```

```{r activity-3b, exercise=TRUE}
#columns that contain the word "time"

```

```{r activity-3c, exercise=TRUE}
#all columns but those that contain the word "time"

```

```{r activity-3d, exercise=TRUE}
#columns that start with the letter "d"

```

## Mutate()

**mutate()** enables you to change the columns available in your original dataset. By using mutate() you can add new columns that are functions of existing columns. For example, in the tax_payers dataset, you note that the salary column is reported in euros rather than dollars. In this case, you want to create a new column that reports salary in euros. To do so you need to multiple the value that are in the original salary by the conversion rate between euros and dollars. --\> **mutate(tax_payers, salary_USD = salary \* 1.21)**. Note how I chose a meaningful name for the new column, and that the conversion rate at the time I created this rmd file was 1.21.

Moreover, keep in mind that mutate() always adds new columns at the end of your dataset. So, to view the new column you can use **View(tax_payers)** or **select(tax_payers, salary_USD)**. Nonetheless, If you want to just use the new compute column you can also use the **transmute()** function (see example below).

```{r}
flights_sml <- select(flights, 
  year:day, 
  ends_with("delay"), 
  distance, 
  air_time
)# what is the purpose of running this?

```

```{r mutate-example1, exercise=TRUE}
# Example 1: using the flights_sml dataset compute a column named gain equal to the difference between departure delay and arrival delay
mutate(
  
# Example 2: using the flights_sml dataset compute a column named gain equal to the difference between departure delay and arrival delay; a column named hours equal to air time dived by 60; and a column named gain_per_hour equal to gain divided by hours.
mutate( # you can refer to column your are just creating, be careful to the order. Can you invert the order of gain_per_hour and gain. Let's try!

# Example 3: using the flights_sml dataset  compute a column named speed equal to the variable distance divided by air time multiplied by 60 (we want speed in mph). Use transmute instead of mutate. What is the difference?
transmute(#If you only want to keep the new variables, use transmute()

```

### Activity 4: Mutate

```{r activity-4a, exercise=TRUE}
#compute a variable that shows the difference between scheduled departure time and the actual time of departure
```

```{r activity-4b, exercise=TRUE}
#compute a variable that shows the air time divided by the total flight distance

```

```{r activity-4c, exercise=TRUE}
#compute a variable that shows the difference between scheduled arrival time and the actual time of arrival

```

```{r activity-4d, exercise=TRUE}
##compute a variable that shows the sum of the departure delay and the air time

```

## Summarise()

**summarise()** enables you to compute descriptive statistics of your dataset. summarise() collapses a dataframe to a single row. For example, if you want to compute the average salary of the observation of the tax payers in your dataset, summarise will return to you one row that contains the average value. --\> **summarise(tax_payers, avg_salary = mean(salary))**. Note how I chose a meaningful name for the output of my summary.

```{r }
summarise(flights, avg_delay = mean(dep_delay, na.rm = TRUE))# we will talk later about NAs (missing values) but the na.rm=TRUE argument is critical if the column you are using for your average contains missing values. Let's remove that argument amd see what happens.

```

```{r  summarise-example1, exercise=TRUE}
# Example 1: compute the mean of the arrival delay column

# Example 2: find the max distance
```

summarise() is not terribly useful unless we pair it with **group_by()** or better it is pretty limited to summarise a single value per column. For example, imagine that you want to see how the average salary of tax payers change depending on their age. In this case, you need to first group_by your dataset using the age column and then compute the average salary. Thanks to the combination of group_by and summarise you will able to explore if the average salary of a 40 years old tax payer is on average higher/lower or equal to the ones of a 21 years old tax payer.

## Summarise + group_by()

So, when you use the group_by() function you can change the unit of analysis from the complete dataset to individual groups (columns that caught your attention). group_by will create a group for each unique value available in the selected column.Then, when you use the dplyr verbs on a grouped dataframe they’ll be automatically applied “by group”. For example, if we applied exactly the same code to a dataframe grouped by date, we get the average delay per date:

```{r}
by_day <- group_by(flights, year, month, day)
summarise(by_day, avg_delay_date = mean(dep_delay, na.rm = TRUE))#help in answer the question about how dates are affecting the delay


```

```{r summarise-example2, exercise=TRUE}
#Example 1: do the same analysis for each airlines. Call the column avg_dalay_carrier.
by_carrier
summarise(#help in answer the question about how the airlines are affecting the delay
```

Together group_by() and summarise() provide one of the tools that you’ll use most commonly when working with dplyr: grouped summaries. These two functions are extremely useful to create descriptives statistics of the column of your dataset.

### Activity 5: Summarise

```{r activity-5a, exercise=TRUE}
#compute the max dep_delay
```

```{r activity-5b, exercise=TRUE}
#compute the min air_time

```

```{r activity-5c, exercise=TRUE}
#compute mean and standard deviation of the distance per each destination

```

```{r activity-5d, exercise=TRUE}
#compute the max and min of air_time per each month

```

### Data Inside R

```{r explore-data, exercise=TRUE}
#Running this code is the equivalent of simply running the name of the dataset you want to print. Meaning default printing will be applied
flights# but what if I want to print in console all columns?
?flights#get info about the dataset
colnames(flights)#get the columns in the dataset
glimpse(flights)#useful way to collapse the content in a summary format
flights |> 
  view()# the other option to print more rows and actually all rows is still the view () function discussed in week 1 to open the dataset in a new window

# " |> " # is the pipe operator and it is used to combine functions and apply them to variables within the tidyverse package. The shortcut for the pipe operator is cmd/ctrl + shift+ M. More of its application later in the class.

```

### Importing data

But what happen when your data come from outside of R? It is time to learn how to import external data. To load flat files in R we will use the readr package, which is part of the core tidyverse package. Most of readr’s functions are concerned with turning flat files into dataframes (e.g, csv files but similar functions exist also for Excel files or other delimited files). These functions all have similar syntax: once you’ve mastered one, you can use the others with ease. In this course we’ll focus on read_csv(). Not only are csv files one of the most common forms of data storage, but once you understand read_csv(), you can easily apply your knowledge to all the other functions in readr.

The first argument to read_csv() is the most important: it’s the path to the file to read. Once again if your file is in your project folder, you will not have any troubles to access it:

```{r importing-data, exercise=TRUE}
heights <- read_csv("heights.csv")#When you run read_csv() it prints out a column specification that gives the name and type of each column. #the heights.csv is available for download on Blackboard in the week 3 module. If this code doesn't work, make sure to move the file in your project folder.
```

#### NOTE

read_csv() uses the first line of the data for the column names, which is a very common convention. The data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings, and instead label them sequentially from X1 to Xn. The function will guess the data type of each column by looking at the first 1000 rows. It is your responsibility to make sure that the columns type are correct. For more info look the parsing sections 11.3 and 11.4. To get other types of data into R, I recommend starting with the tidyverse packages listed below:

-   haven package: reads SPSS, Stata, and SAS files.

-   readxl package: reads excel files (both .xls and .xlsx).

-   DBI package: along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame.

### Writing data

Now that we have seen how to bring data into RStudio, what about exporting data from RStudio? The readr package comes also with two useful functions for writing data back to disk: write_csv() and write_tsv(). If you want to export a csv file to Excel, use write_excel_csv(). The most important arguments are x (the data frame to save), and path (the location to save it).

```{r exporting-data, exercise=TRUE}
write_csv(nycflights13::flights, "flights.csv")#I am exporting the flights dataset and writing it into a csv file called flights.csv. After running the code that file will be available in your working directory (which should be your project folder)
```

## Pipes

You probably have seen the pipe symbol before but let's learn how it works and why it is so important in tidyverse. Before we do that please keep in mind that two types of pipes exist:

-   The magrittr pipe %\>% that comes from the magrittr package created by Stefan Milton Bache. Packages in the tidyverse load %\>% for you automatically, so you don’t usually load magrittr explicitly. While magrittr pipe was used for a while in the tidyverse world, it is now losing its traction.
-   In fact, the native pipe \|\> is becoming more popular and it is most commonly used.

For this course we will use only the native pipe \|\>. While for simple cases, \|\> and %\>% behave identically only the native pipe is part of base R, and so it’s always available for you to use, even when you’re not loading the tidyverse. Moreover, \|\> is simpler than %\>% and it works better with more advanced tasks.

However, you might need to make one change to your RStudio options to use \|\> instead of %\>% by accessing the Code Editing tab of your Project Option; After you made this change you can add the native pipe to your code by using the built-in keyboard shortcut Ctrl/Cmd + Shift + M.

### Combining multiple operations with the pipe

Imagine that we want to explore the relationship between the distance and average delay for each destination. There are three steps to prepare our original flights data:

-   Group flights by destination.

-   Summarise to compute average distance, average delay, and number of flights.

-   Filter to remove noisy points (less than 20 observations, small sample) and Honolulu airport (almost twice as far away as the next closest airport).

If we put in practice what we learned so far, this code is a little frustrating to write because we have to give each intermediate data frame a name, even though we don’t care about it. Naming things is hard, so this slows down our analysis.

```{r pipe-1, exercise=TRUE}
by_dest <- group_by(flights, dest)
```

```{r pipe-2, exercise=TRUE}
delay <- summarise(by_dest,
  count = n(),
  avg_dist = mean(distance, na.rm = TRUE),
  avg_delay = mean(arr_delay, na.rm = TRUE)
)#know how many flights go to each destination [n()]; average distance and average delay for flights at each destination
```

```{r pipe-3, exercise=TRUE}
delay_filtered <- filter(delay, count > 20, dest != "HNL")#now let's remove all destinations that have less or equal to 20 flights and let's exclude Honululu from it. Notice how many assignment I need to create the desired object (3)

```

There’s another, much simpler and efficient, way to tackle the same problem thanks to the pipe operator, \|\>, because we can combine manipulations together:

```{r piping-together, exercise=TRUE}
filtered_delay <- flights |> 
  group_by(dest) |> 
  summarise(
    count = n(),
    avg_dist = mean(distance, na.rm = TRUE),
    avg_delay = mean(arr_delay, na.rm = TRUE)
  ) |> 
  filter(count > 20, dest != "HNL") #Notice how many assignment I need to create the desired object (1), two less than the previous code.

```

Anyway, what is great about using pipe is that your code now focuses on the transformations, and not anymore on what’s being transformed, which makes the code easier to read. In fact, the best way to read the above code is to take each line before the pipe as an imperative statement (remember that the dataset us always the starting point). - Take the flights dataset, then - Group it by destination, then - summarise it (I want the number of flights at each destination, the average distance and the average delay of each destination), then - Filter the results and include only those destinations with more than 20 flights and exclude Honululu.

As suggested by this reading, a good way to pronounce \|\> when reading code is “then”. So, remember that you can use the pipe to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. We’ll use piping from now on because it considerably improves the readability of code and it makes your code more efficient. Working with the pipe is one of the key criteria for belonging to the tidyverse world!

By the way we can use the pipe also to what we have already learned ;-)

### Activity 6: Basic pipes

```{r activity-6a, exercise=TRUE}
# keep only the flights in February with more than 75 minutes delay
```

```{r activity-6b, exercise=TRUE}
# sort the flights by air_time from bigger to smaller

```

```{r activity-6c, exercise=TRUE}
# keep all columns but those that contain the word "dep"

```

```{r activity-6d, exercise=TRUE}
# compute a column named gain equal to the difference between departure delay and arrival delay

```

### Missing values

Ok now it is time to provide an explanation on the **na.rm** argument that we used in the summarise function. Let's try one more time in not using it. What happens if we don’t have it?

```{r}
flights |> 
  group_by(year, month, day) |> 
  summarise(mean = mean(dep_delay))
```

We get a lot of missing values! That’s because aggregation functions obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. Fortunately, all aggregation functions have an na.rm argument which removes the missing values prior to computation:

```{r}
flights |> 
  group_by(year, month, day) |> 
  summarise(mean = mean(dep_delay, na.rm = TRUE))
```

In this case, missing values represent cancelled flights. So, we could also tackle the problem by first removing all the cancelled flights. By doing so we get rid of all missing values and so of the need of using the na.rm argument.

```{r}
not_cancelled <- flights |> 
  filter(!is.na(dep_delay), !is.na(arr_delay))#save this dataset so we can reuse it in the future.

```

```{r}
not_cancelled |> 
  group_by(year, month, day) |> 
  summarise(mean = mean(dep_delay))#no need of na.rm since we don't have NAs anymore
```

### Why producing counts matters?

We have already seen how to count observations. However, it is extremely important that you keep in mind that whenever you do any aggregation, it’s always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data. Results and conclusions based on few observations contain noise and are drawn based on a small number of events and can lead to incorrect insights.

*Imaginary Scenario*: Let's talk about flight delays and why we can't always compare them from different times like before, during, and after the pandemic. Things change, and comparisons might not be fair.

Imagine a new direct flight from DeKalb to Potenza. On its first trip, something rare happens: a passenger gets sick, and because of a new rule, the flight is delayed for 14 hours while they wait for a health check.

If we only look at this one flight, we might think the average delay for this route is 14 hours. But that's not true, it's just one unusual case. We can't say all flights on this route are always delayed like this based on one incident. We need more flights to make a fair judgment. If after many flights the delay is still long, then we might think it's a bad route. But for now, it's too early to decide based on just one flight.

I hope the scenario made the point on the importance of count clear. Now let's use a real data example and let’s look at the planes (identified by their tail number) that have the highest average delays:

```{r producing-counts-1, exercise=TRUE}
not_cancelled |> 
  group_by(tailnum) |> 
  summarise(
    delay = mean(dep_delay))# no info about how many flights each tailnum has made. Which one seem the most problematic? 

```

```{r producing-counts-2, exercise=TRUE}
not_cancelled |> 
  group_by(tailnum) |> 
  summarise(
    delay = mean(dep_delay, na.rm = TRUE), n = n())#now you also now on how many flights your average is based on. You don't want to base any conclusion on a small number of observation. Check the # of flights for the tail number D942DN
```

### Recap of the summary functions

Just using means and counts can get you a long way, but R provides many other useful summary functions that should be taken in consideration when producing descriptive statistics. Here is a list of the most useful ones with many opportunities to practice them in the activities belpw:

-   Measures of location: mean(x) and median(x) . The mean is the sum divided by the length; the median is a value where 50% of x is above it, and 50% is below it.

-   Measures of spread: sd(x). The root mean squared deviation, or standard deviation sd(x), is the standard measure of spread.

-   Measures of rank: min(x) and max(x). The min will help you identify the smallest value, while the max allows you to find the largest value in column.

```{r measures-location, exercise=TRUE}
#measures of location example
not_cancelled |> 
  group_by(year, month, day) |> 
  summarise(
    avg_delay = mean(dep_delay),
    median_delay = median(dep_delay)) # the average positive delay

```

```{r measures-spread, exercise=TRUE}
#measures of spread example
not_cancelled |> 
  group_by(dest) |> 
  summarise(distance_sd = sd(distance)) |> 
  arrange(desc(distance_sd))# Is distance to some destinations more variable than to others?
```

```{r measure-rank, exercise=TRUE}
#measures of rank example
not_cancelled |> 
  group_by(year, month, day) |> 
  summarise(
    first = min(dep_time),
    last = max(dep_time)
  )# When do the first and last flights leave each day?
```

### Activity 7: Summary functions p1 (must use the pipe)

```{r activity-7a, exercise=TRUE}
#Compute the median of the distance variable per each tail number?
```

```{r activity-7b, exercise=TRUE}
#Compute the median of the arr_delay variable per each carrier?

```

```{r activity-7c, exercise=TRUE}
#Compute the mean and median of the dep_delay variable per each dest?

```

```{r activity-7d, exercise=TRUE}
#Compute the mean and median of the air_time variable per each origin?

```

### Activity 8: Summary functions p2 (must use the pipe)

```{r activity-8a, exercise=TRUE}
# Find min and max of the distance variable for each tail number?
```

```{r activity-8b, exercise=TRUE}
# Find min, max and sd of the air_time variable for each carrier?

```

```{r activity-8c, exercise=TRUE}
# Find min and max of the dep_delay variable for each month?

```

```{r activity-8d, exercise=TRUE}
# Find min, max and sd of the arr_delay variable for each destination?

```

##### Challenge1: You must use the pipe to complete the challenge and not create any intermediate objects (one big chunk of code)

```{r challenge1, exercise=TRUE}
#Use the flights dataset and apply the following manipulations at the same time using pipes:
#1) Make sure your dataset has only the following columns: month, day, dep_delay, arr_delay, dest, distance, carrier and air_time
#2) Reorder your data and show them from the highest arr_delay flight to the smallest one.
#3) Create a column named distance_km that is equal to distance/1.6
#4) Per each carrier compute the avg_arr_delay, min_arr_delay, max_arr_delay, sd_arr_delay, median_arr_delay and the number of flights operated.
#5) Keep in the output only the 5 carriers with the lowest median_arr_delay

```

##### Challenge2: You must use the pipe to complete the challenge and not create any intermediate objects (one big chunk of code)

```{r challenge2, exercise=TRUE}
#Use the flights dataset and apply the following manipulations at the same time using pipes:
#1) Keep only flights departed from JFK in February
#2) Make sure your dataset has only the following columns: day, dep_delay, arr_delay, dest,and carrier 
#3) Create a column named final_delay that is equal to arr_delay- dep_delay
#4) Per each dest compute the avg_final_delay, min_final_delay, max_final_delay, sd_final_delay, median_final_delay and the number of flights landed there.
#5) Reorder your data and show them from the highest median_final_delay to the smallest one.

```

### When not to use the pipe

The pipe operator is a powerful tool, but it’s not the only tool at your disposal, and it doesn’t solve every problem! Pipes are most useful for rewriting a fairly short linear sequence of operations. I think you should reach for another tool when:

-   Your pipes are longer than (say) ten steps. In that case, create intermediate objects with meaningful names. That will make debugging easier, because you can more easily check the intermediate results, and it makes it easier to understand your code, because the variable names can help communicate intent.

-   You have multiple inputs or outputs. If there isn’t one primary object being transformed, but two or more objects being combined together, don’t use the pipe.

## Now that you know how to manipulate your data.What about learning how to visualize them? See you for more in the next chapter!

